{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Building a Transformer with PyTorch"
      ],
      "metadata": {
        "id": "CWr08iFugRIj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setting up PyTorch\n",
        "\n",
        "Before diving into building a Transformer, it is essential to set up the working environment correctly. First and foremost, PyTorch needs to be installed. PyTorch (current stable version - 2.0.1) can be easily installed through pip or conda package managers.\n",
        "\n",
        "For pip, use the command:\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "sYFIqRsWgR8-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip3 install torch torchvision torchaudio"
      ],
      "metadata": {
        "id": "WoIixXHmgWmY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conda install pytorch torchvision torchaudio pytorch-cuda=11.7 -c pytorch -c nvidia"
      ],
      "metadata": {
        "id": "6fp9lfLXgZ4H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Building the Transformer Model with PyTorch\n",
        "\n",
        "To build the Transformer model the following steps are necessary:\n",
        "\n",
        "Importing the libraries and modules\n",
        "Defining the basic building blocks - Multi-head Attention, Position-Wise Feed-Forward Networks, Positional Encoding\n",
        "Building the Encoder block\n",
        "Building the Decoder block\n",
        "Combining the Encoder and Decoder layers to create the complete Transformer network\n",
        "1. Importing the necessary libraries and modules\n",
        "\n",
        "We’ll start with importing the PyTorch library for core functionality, the neural network module for creating neural networks, the optimization module for training networks, and the data utility functions for handling data. Additionally, we’ll import the standard Python math module for mathematical operations and the copy module for creating copies of complex objects.\n",
        "\n",
        "These tools set the foundation for defining the model's architecture, managing data, and establishing the training process."
      ],
      "metadata": {
        "id": "qwF51w4Hgcz-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "import math\n",
        "import copy"
      ],
      "metadata": {
        "id": "Q7AUVFccgfve"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Defining the basic building blocks: Multi-Head Attention, Position-wise Feed-Forward Networks, Positional Encoding\n",
        "\n",
        "Multi-head Attention\n",
        "\n",
        "The Multi-Head Attention mechanism computes the attention between each pair of positions in a sequence. It consists of multiple “attention heads” that capture different aspects of the input sequence.\n",
        "\n",
        "To know more about Multi-Head Attention, check out this Attention mechanisms section of the Large Language Models (LLMs) Concepts course."
      ],
      "metadata": {
        "id": "z2M2HBpAghTv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        # Ensure that the model dimension (d_model) is divisible by the number of heads\n",
        "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
        "\n",
        "        # Initialize dimensions\n",
        "        self.d_model = d_model # Model's dimension\n",
        "        self.num_heads = num_heads # Number of attention heads\n",
        "        self.d_k = d_model // num_heads # Dimension of each head's key, query, and value\n",
        "\n",
        "        # Linear layers for transforming inputs\n",
        "        self.W_q = nn.Linear(d_model, d_model) # Query transformation\n",
        "        self.W_k = nn.Linear(d_model, d_model) # Key transformation\n",
        "        self.W_v = nn.Linear(d_model, d_model) # Value transformation\n",
        "        self.W_o = nn.Linear(d_model, d_model) # Output transformation\n",
        "\n",
        "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
        "        # Calculate attention scores\n",
        "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
        "\n",
        "        # Apply mask if provided (useful for preventing attention to certain parts like padding)\n",
        "        if mask is not None:\n",
        "            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n",
        "\n",
        "        # Softmax is applied to obtain attention probabilities\n",
        "        attn_probs = torch.softmax(attn_scores, dim=-1)\n",
        "\n",
        "        # Multiply by values to obtain the final output\n",
        "        output = torch.matmul(attn_probs, V)\n",
        "        return output\n",
        "\n",
        "    def split_heads(self, x):\n",
        "        # Reshape the input to have num_heads for multi-head attention\n",
        "        batch_size, seq_length, d_model = x.size()\n",
        "        return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)\n",
        "\n",
        "    def combine_heads(self, x):\n",
        "        # Combine the multiple heads back to original shape\n",
        "        batch_size, _, seq_length, d_k = x.size()\n",
        "        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n",
        "\n",
        "    def forward(self, Q, K, V, mask=None):\n",
        "        # Apply linear transformations and split heads\n",
        "        Q = self.split_heads(self.W_q(Q))\n",
        "        K = self.split_heads(self.W_k(K))\n",
        "        V = self.split_heads(self.W_v(V))\n",
        "\n",
        "        # Perform scaled dot-product attention\n",
        "        attn_output = self.scaled_dot_product_attention(Q, K, V, mask)\n",
        "\n",
        "        # Combine heads and apply output transformation\n",
        "        output = self.W_o(self.combine_heads(attn_output))\n",
        "        return output"
      ],
      "metadata": {
        "id": "PGTdZgnVglkI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads):"
      ],
      "metadata": {
        "id": "wbj-DF-TgnjD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def scaled_dot_product_attention(self, Q, K, V, mask=None):"
      ],
      "metadata": {
        "id": "bK80i1jigvfF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_heads(self, x):"
      ],
      "metadata": {
        "id": "5DlqENFwgxeG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def combine_heads(self, x):"
      ],
      "metadata": {
        "id": "4BvIY7Bog18y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def forward(self, Q, K, V, mask=None):"
      ],
      "metadata": {
        "id": "c0SgILyTg6n6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionWiseFeedForward(nn.Module):\n",
        "    def __init__(self, d_model, d_ff):\n",
        "        super(PositionWiseFeedForward, self).__init__()\n",
        "        self.fc1 = nn.Linear(d_model, d_ff)\n",
        "        self.fc2 = nn.Linear(d_ff, d_model)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc2(self.relu(self.fc1(x)))"
      ],
      "metadata": {
        "id": "DdkE1JXpg8Cw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionWiseFeedForward(nn.Module):"
      ],
      "metadata": {
        "id": "YN6WxsnFg9-7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def __init__(self, d_model, d_ff):\n",
        "    super(PositionWiseFeedForward, self).__init__()\n",
        "    self.fc1 = nn.Linear(d_model, d_ff)\n",
        "    self.fc2 = nn.Linear(d_ff, d_model)\n",
        "    self.relu = nn.ReLU()"
      ],
      "metadata": {
        "id": "XzCb_CVkhAFY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def forward(self, x):\n",
        "    return self.fc2(self.relu(self.fc1(x)))"
      ],
      "metadata": {
        "id": "u2CdwoxuhCT0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_seq_length):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "\n",
        "        pe = torch.zeros(max_seq_length, d_model)\n",
        "        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
        "\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        self.register_buffer('pe', pe.unsqueeze(0))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:, :x.size(1)]"
      ],
      "metadata": {
        "id": "539c07WehDsD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):"
      ],
      "metadata": {
        "id": "BiQBHYCahGsX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def __init__(self, d_model, max_seq_length):\n",
        "    super(PositionalEncoding, self).__init__()\n",
        "\n",
        "    pe = torch.zeros(max_seq_length, d_model)\n",
        "    position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n",
        "    div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
        "\n",
        "    pe[:, 0::2] = torch.sin(position * div_term)\n",
        "    pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "    self.register_buffer('pe', pe.unsqueeze(0))"
      ],
      "metadata": {
        "id": "uVUs9ztShI80"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def forward(self, x):\n",
        "    return x + self.pe[:, :x.size(1)]"
      ],
      "metadata": {
        "id": "i0PGs2jahLZ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        attn_output = self.self_attn(x, x, x, mask)\n",
        "        x = self.norm1(x + self.dropout(attn_output))\n",
        "        ff_output = self.feed_forward(x)\n",
        "        x = self.norm2(x + self.dropout(ff_output))\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "W4aEsVoBhOb6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderLayer(nn.Module):"
      ],
      "metadata": {
        "id": "1rUB-FHbhRZ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def __init__(self, d_model, num_heads, d_ff, dropout):\n",
        "    super(EncoderLayer, self).__init__()\n",
        "    self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
        "    self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
        "    self.norm1 = nn.LayerNorm(d_model)\n",
        "    self.norm2 = nn.LayerNorm(d_model)\n",
        "    self.dropout = nn.Dropout(dropout)"
      ],
      "metadata": {
        "id": "xOfqZhYRhTJc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def forward(self, x, mask):\n",
        "    attn_output = self.self_attn(x, x, x, mask)\n",
        "    x = self.norm1(x + self.dropout(attn_output))\n",
        "    ff_output = self.feed_forward(x)\n",
        "    x = self.norm2(x + self.dropout(ff_output))\n",
        "    return x"
      ],
      "metadata": {
        "id": "d9SykujVhVGn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.norm3 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, enc_output, src_mask, tgt_mask):\n",
        "        attn_output = self.self_attn(x, x, x, tgt_mask)\n",
        "        x = self.norm1(x + self.dropout(attn_output))\n",
        "        attn_output = self.cross_attn(x, enc_output, enc_output, src_mask)\n",
        "        x = self.norm2(x + self.dropout(attn_output))\n",
        "        ff_output = self.feed_forward(x)\n",
        "        x = self.norm3(x + self.dropout(ff_output))\n",
        "        return x"
      ],
      "metadata": {
        "id": "_XPxfX_FhXgY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderLayer(nn.Module):"
      ],
      "metadata": {
        "id": "ddoPOAvQhaYI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def __init__(self, d_model, num_heads, d_ff, dropout):\n",
        "    super(DecoderLayer, self).__init__()\n",
        "    self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
        "    self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
        "    self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
        "    self.norm1 = nn.LayerNorm(d_model)\n",
        "    self.norm2 = nn.LayerNorm(d_model)\n",
        "    self.norm3 = nn.LayerNorm(d_model)\n",
        "    self.dropout = nn.Dropout(dropout)"
      ],
      "metadata": {
        "id": "1HG7CwavhcVf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ef forward(self, x, enc_output, src_mask, tgt_mask):\n",
        "    attn_output = self.self_attn(x, x, x, tgt_mask)\n",
        "    x = self.norm1(x + self.dropout(attn_output))\n",
        "    attn_output = self.cross_attn(x, enc_output, enc_output, src_mask)\n",
        "    x = self.norm2(x + self.dropout(attn_output))\n",
        "    ff_output = self.feed_forward(x)\n",
        "    x = self.norm3(x + self.dropout(ff_output))\n",
        "    return x"
      ],
      "metadata": {
        "id": "KZfDH7y4heX2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout):\n",
        "        super(Transformer, self).__init__()\n",
        "        self.encoder_embedding = nn.Embedding(src_vocab_size, d_model)\n",
        "        self.decoder_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
        "        self.positional_encoding = PositionalEncoding(d_model, max_seq_length)\n",
        "\n",
        "        self.encoder_layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
        "        self.decoder_layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
        "\n",
        "        self.fc = nn.Linear(d_model, tgt_vocab_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def generate_mask(self, src, tgt):\n",
        "        src_mask = (src != 0).unsqueeze(1).unsqueeze(2)\n",
        "        tgt_mask = (tgt != 0).unsqueeze(1).unsqueeze(3)\n",
        "        seq_length = tgt.size(1)\n",
        "        nopeak_mask = (1 - torch.triu(torch.ones(1, seq_length, seq_length), diagonal=1)).bool()\n",
        "        tgt_mask = tgt_mask & nopeak_mask\n",
        "        return src_mask, tgt_mask\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        src_mask, tgt_mask = self.generate_mask(src, tgt)\n",
        "        src_embedded = self.dropout(self.positional_encoding(self.encoder_embedding(src)))\n",
        "        tgt_embedded = self.dropout(self.positional_encoding(self.decoder_embedding(tgt)))\n",
        "\n",
        "        enc_output = src_embedded\n",
        "        for enc_layer in self.encoder_layers:\n",
        "            enc_output = enc_layer(enc_output, src_mask)\n",
        "\n",
        "        dec_output = tgt_embedded\n",
        "        for dec_layer in self.decoder_layers:\n",
        "            dec_output = dec_layer(dec_output, enc_output, src_mask, tgt_mask)\n",
        "\n",
        "        output = self.fc(dec_output)\n",
        "        return output"
      ],
      "metadata": {
        "id": "InWJqqi-hgzj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Class Definition:\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "Initialization:\n",
        "\n",
        "def __init__(self, src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout):\n",
        "The constructor takes the following parameters:\n",
        "\n",
        "src_vocab_size: Source vocabulary size.\n",
        "tgt_vocab_size: Target vocabulary size.\n",
        "d_model: The dimensionality of the model's embeddings.\n",
        "num_heads: Number of attention heads in the multi-head attention mechanism.\n",
        "num_layers: Number of layers for both the encoder and the decoder.\n",
        "d_ff: Dimensionality of the inner layer in the feed-forward network.\n",
        "max_seq_length: Maximum sequence length for positional encoding.\n",
        "dropout: Dropout rate for regularization.\n",
        "And it defines the following components:\n",
        "\n",
        "self.encoder_embedding: Embedding layer for the source sequence.\n",
        "self.decoder_embedding: Embedding layer for the target sequence.\n",
        "self.positional_encoding: Positional encoding component.\n",
        "self.encoder_layers: A list of encoder layers.\n",
        "self.decoder_layers: A list of decoder layers.\n",
        "self.fc: Final fully connected (linear) layer mapping to target vocabulary size.\n",
        "self.dropout: Dropout layer.\n",
        "Generate Mask Method:\n",
        "\n",
        "def generate_mask(self, src, tgt):\n",
        "This method is used to create masks for the source and target sequences, ensuring that padding tokens are ignored and that future tokens are not visible during training for the target sequence.\n",
        "\n",
        "Forward Method:\n",
        "\n",
        "def forward(self, src, tgt):\n",
        "This method defines the forward pass for the Transformer, taking source and target sequences and producing the output predictions.\n",
        "\n",
        "Input Embedding and Positional Encoding: The source and target sequences are first embedded using their respective embedding layers and then added to their positional encodings.\n",
        "Encoder Layers: The source sequence is passed through the encoder layers, with the final encoder output representing the processed source sequence.\n",
        "Decoder Layers: The target sequence and the encoder's output are passed through the decoder layers, resulting in the decoder's output.\n",
        "Final Linear Layer: The decoder's output is mapped to the target vocabulary size using a fully connected (linear) layer.\n",
        "Output:\n",
        "\n",
        "The final output is a tensor representing the model's predictions for the target sequence.\n",
        "\n",
        "Summary:\n",
        "\n",
        "The Transformer class brings together the various components of a Transformer model, including the embeddings, positional encoding, encoder layers, and decoder layers. It provides a convenient interface for training and inference, encapsulating the complexities of multi-head attention, feed-forward networks, and layer normalization.\n",
        "\n",
        "This implementation follows the standard Transformer architecture, making it suitable for sequence-to-sequence tasks like machine translation, text summarization, etc. The inclusion of masking ensures that the model adheres to the causal dependencies within sequences, ignoring padding tokens and preventing information leakage from future tokens.\n",
        "\n",
        "These sequential steps empower the Transformer model to efficiently process input sequences and produce corresponding output sequences."
      ],
      "metadata": {
        "id": "RzGMFb0shlFN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training the PyTorch Transformer Model\n",
        "\n",
        "Sample data preparation\n",
        "\n",
        "For illustrative purposes, a dummy dataset will be crafted in this example. However, in a practical scenario, a more substantial dataset would be employed, and the process would involve text preprocessing along with the creation of vocabulary mappings for both the source and target languages.\n",
        "\n",
        "src_vocab_size = 5000\n",
        "tgt_vocab_size = 5000\n",
        "d_model = 512\n",
        "num_heads = 8\n",
        "num_layers = 6\n",
        "d_ff = 2048\n",
        "max_seq_length = 100\n",
        "dropout = 0.1\n",
        "\n",
        "transformer = Transformer(src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout)\n",
        "\n",
        "# Generate random sample data\n",
        "src_data = torch.randint(1, src_vocab_size, (64, max_seq_length))  # (batch_size, seq_length)\n",
        "tgt_data = torch.randint(1, tgt_vocab_size, (64, max_seq_length))  # (batch_size, seq_length)\n",
        "Hyperparameters:\n",
        "\n",
        "These values define the architecture and behavior of the transformer model:\n",
        "\n",
        "src_vocab_size, tgt_vocab_size: Vocabulary sizes for source and target sequences, both set to 5000.\n",
        "d_model: Dimensionality of the model's embeddings, set to 512.\n",
        "num_heads: Number of attention heads in the multi-head attention mechanism, set to 8.\n",
        "num_layers: Number of layers for both the encoder and the decoder, set to 6.\n",
        "d_ff: Dimensionality of the inner layer in the feed-forward network, set to 2048.\n",
        "max_seq_length: Maximum sequence length for positional encoding, set to 100.\n",
        "dropout: Dropout rate for regularization, set to 0.1.\n",
        "Creating a Transformer Instance:\n",
        "\n",
        "transformer = Transformer(src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout)\n",
        "This line creates an instance of the Transformer class, initializing it with the given hyperparameters. The instance will have the architecture and behavior defined by these hyperparameters.\n",
        "\n",
        "Generating Random Sample Data:\n",
        "\n",
        "The following lines generate random source and target sequences:\n",
        "\n",
        "src_data: Random integers between 1 and src_vocab_size, representing a batch of source sequences with shape (64, max_seq_length).\n",
        "tgt_data: Random integers between 1 and tgt_vocab_size, representing a batch of target sequences with shape (64, max_seq_length).\n",
        "These random sequences can be used as inputs to the transformer model, simulating a batch of data with 64 examples and sequences of length 100.\n",
        "Summary:\n",
        "\n",
        "The code snippet demonstrates how to initialize a transformer model and generate random source and target sequences that can be fed into the model. The chosen hyperparameters determine the specific structure and properties of the transformer. This setup could be part of a larger script where the model is trained and evaluated on actual sequence-to-sequence tasks, such as machine translation or text summarization.\n",
        "\n",
        "Training the Model\n",
        "\n",
        "Next, the model will be trained utilizing the aforementioned sample data. However, in a real-world scenario, a significantly larger dataset would be employed, which would typically be partitioned into distinct sets for training and validation purposes.\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
        "optimizer = optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
        "\n",
        "transformer.train()\n",
        "\n",
        "for epoch in range(100):\n",
        "    optimizer.zero_grad()\n",
        "    output = transformer(src_data, tgt_data[:, :-1])\n",
        "    loss = criterion(output.contiguous().view(-1, tgt_vocab_size), tgt_data[:, 1:].contiguous().view(-1))\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    print(f\"Epoch: {epoch+1}, Loss: {loss.item()}\")\n",
        "Loss Function and Optimizer:\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0): Defines the loss function as cross-entropy loss. The ignore_index argument is set to 0, meaning the loss will not consider targets with an index of 0 (typically reserved for padding tokens).\n",
        "optimizer = optim.Adam(...): Defines the optimizer as Adam with a learning rate of 0.0001 and specific beta values.\n",
        "Model Training Mode:\n",
        "\n",
        "transformer.train(): Sets the transformer model to training mode, enabling behaviors like dropout that only apply during training.\n",
        "Training Loop:\n",
        "\n",
        "The code snippet trains the model for 100 epochs using a typical training loop:\n",
        "\n",
        "for epoch in range(100): Iterates over 100 training epochs.\n",
        "optimizer.zero_grad(): Clears the gradients from the previous iteration.\n",
        "output = transformer(src_data, tgt_data[:, :-1]): Passes the source data and the target data (excluding the last token in each sequence) through the transformer. This is common in sequence-to-sequence tasks where the target is shifted by one token.\n",
        "loss = criterion(...): Computes the loss between the model's predictions and the target data (excluding the first token in each sequence). The loss is calculated by reshaping the data into one-dimensional tensors and using the cross-entropy loss function.\n",
        "loss.backward(): Computes the gradients of the loss with respect to the model's parameters.\n",
        "optimizer.step(): Updates the model's parameters using the computed gradients.\n",
        "print(f\"Epoch: {epoch+1}, Loss: {loss.item()}\"): Prints the current epoch number and the loss value for that epoch.\n",
        "Summary:\n",
        "\n",
        "This code snippet trains the transformer model on randomly generated source and target sequences for 100 epochs. It uses the Adam optimizer and the cross-entropy loss function. The loss is printed for each epoch, allowing you to monitor the training progress. In a real-world scenario, you would replace the random source and target sequences with actual data from your task, such as machine translation."
      ],
      "metadata": {
        "id": "SeqUnOCDhs1J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transformer Model Performance Evaluation\n",
        "\n",
        "After training the model, its performance can be evaluated on a validation dataset or test dataset. The following is an example of how this could be done:\n",
        "\n",
        "transformer.eval()\n",
        "\n",
        "# Generate random sample validation data\n",
        "val_src_data = torch.randint(1, src_vocab_size, (64, max_seq_length))  # (batch_size, seq_length)\n",
        "val_tgt_data = torch.randint(1, tgt_vocab_size, (64, max_seq_length))  # (batch_size, seq_length)\n",
        "\n",
        "with torch.no_grad():\n",
        "\n",
        "    val_output = transformer(val_src_data, val_tgt_data[:, :-1])\n",
        "    val_loss = criterion(val_output.contiguous().view(-1, tgt_vocab_size), val_tgt_data[:, 1:].contiguous().view(-1))\n",
        "    print(f\"Validation Loss: {val_loss.item()}\")\n",
        "Evaluation Mode:\n",
        "\n",
        "transformer.eval(): Puts the transformer model in evaluation mode. This is important because it turns off certain behaviors like dropout that are only used during training.\n",
        "Generate Random Validation Data:\n",
        "\n",
        "val_src_data: Random integers between 1 and src_vocab_size, representing a batch of validation source sequences with shape (64, max_seq_length).\n",
        "val_tgt_data: Random integers between 1 and tgt_vocab_size, representing a batch of validation target sequences with shape (64, max_seq_length).\n",
        "Validation Loop:\n",
        "\n",
        "with torch.no_grad(): Disables gradient computation, as we don't need to compute gradients during validation. This can reduce memory consumption and speed up computations.\n",
        "val_output = transformer(val_src_data, val_tgt_data[:, :-1]): Passes the validation source data and the validation target data (excluding the last token in each sequence) through the transformer.\n",
        "val_loss = criterion(...): Computes the loss between the model's predictions and the validation target data (excluding the first token in each sequence). The loss is calculated by reshaping the data into one-dimensional tensors and using the previously defined cross-entropy loss function.\n",
        "print(f\"Validation Loss: {val_loss.item()}\"): Prints the validation loss value.\n",
        "Summary:\n",
        "\n",
        "This code snippet evaluates the transformer model on a randomly generated validation dataset, computes the validation loss, and prints it. In a real-world scenario, the random validation data should be replaced with actual validation data from the task you are working on. The validation loss can give you an indication of how well your model is performing on unseen data, which is a critical measure of the model's generalization ability."
      ],
      "metadata": {
        "id": "cGuJyhMGh0y0"
      }
    }
  ]
}